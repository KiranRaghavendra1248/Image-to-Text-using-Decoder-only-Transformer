# Image-to-Text
<ul>
  <li> Implemented Encoder-Decoder architecture for image captioning on MS COCO dataset</li>
  <li> Used SOTA Deep CNN models as Image Encoder and GPT-like Decoder only Transformer architecture with Masked Attention and Cross Attention as Decoder</li>
  <li> Explored Dynamic Quantization i.e. Post Training Quantization of nn.Linear, nn.Conv2d, nn.ReLU layers in Encoder-Decoder Architecture</li>
  <li> Performed quantization from float32 to int8, resulting in reduction of model size by 30% and inference latency by 5%</li>
  <li> Refer attached jupyter notebook</li>
</ul>
